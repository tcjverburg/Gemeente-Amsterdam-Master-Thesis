{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import sys\n",
    "import re\n",
    "import itertools\n",
    "import operator\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "\n",
    "from gensim.sklearn_api import D2VTransformer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.feature_selection import mutual_info_classif, SelectKBest, chi2\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, ParameterGrid, cross_validate\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline, make_pipeline, FeatureUnion\n",
    "\n",
    "base = os.getcwd().split('Master-Thesis')[0].replace('\\\\', '/')\n",
    "sys.path.insert(0, base + '/Master-Thesis/research/pre-processing')\n",
    "\n",
    "from pre_processing_functions import *\n",
    "from model_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Paths of the annotations and dataset\n",
    "path_df_anno = base + '/Master-Thesis/research/pre-processing/complete_annotations.pickle'\n",
    "path_dataset = base + '/Master-Thesis/research/pre-processing/final_dataset.pickle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the data and annotations in, as well as define the test set\n",
    "df_annotations = pd.read_pickle(path_df_anno)\n",
    "\n",
    "df_final= pd.read_pickle(path_dataset)\n",
    "df_test = df_final[int(0.8*len(df_final)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the predictions of RQ3.ipynb that were saved for analysis\n",
    "prediction_ngram = np.array(pd.read_csv('predictionngram.csv'))[0][1:]\n",
    "prediction_tfidf = np.array(pd.read_csv('predictiontfidf.csv'))[0][1:]\n",
    "prediction_d2v = np.array(pd.read_csv('predictiond2v.csv'))[0][1:]\n",
    "labels_test = df_test.check_relevant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## False Negative and True Positive "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check whether a prediction is a TP or a FN for all methods and save index to list\n",
    "TP_ngram, FN_ngram = TPFN(prediction_ngram, labels_test)\n",
    "TP_tfidf, FN_tfidf = TPFN(prediction_tfidf, labels_test)\n",
    "TP_d2v, FN_d2v = TPFN(prediction_d2v, labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the overlap of all the TP and FN between the three different vectorization methods\n",
    "all_FN_list = list(set([FN_d2v, FN_tfidf, FN_ngram][0]).intersection(*[FN_d2v, FN_tfidf, FN_ngram]))\n",
    "all_TP_list = list(set([TP_d2v, TP_tfidf, TP_ngram][0]).intersection(*[TP_d2v, TP_tfidf, TP_ngram]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the rows of the dataframe based on the indexes as determined earlier for TP and FN seperately\n",
    "all_FN_df = df_test.iloc[all_FN_list,:]\n",
    "all_TP_df = df_test.iloc[all_TP_list,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create seperate datataframes for the different document types for the TP and FN dataframe\n",
    "all_TP_df_uitgifte = all_TP_df[(all_TP_df.levering != 1)& (all_TP_df.splitsing != 1)& (all_TP_df.uitgifte == 1)]\n",
    "all_TP_df_splitsing = all_TP_df[(all_TP_df.levering != 1)& (all_TP_df.splitsing == 1)& (all_TP_df.uitgifte != 1)]\n",
    "all_TP_df_levering = all_TP_df[(all_TP_df.levering == 1)& (all_TP_df.splitsing != 1)& (all_TP_df.uitgifte != 1)]\n",
    "\n",
    "all_FN_df_uitgifte = all_FN_df[(all_FN_df.levering != 1)& (all_FN_df.splitsing != 1)& (all_FN_df.uitgifte == 1)]\n",
    "all_FN_df_splitsing = all_FN_df[(all_FN_df.levering != 1)& (all_FN_df.splitsing == 1)& (all_FN_df.uitgifte != 1)]\n",
    "all_FN_df_levering = all_FN_df[(all_FN_df.levering == 1)& (all_FN_df.splitsing != 1)& (all_FN_df.uitgifte != 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge the individual highlights with corresponding pages from dataset\n",
    "df_annotations['filename'] = df_annotations['filename'].apply(lambda x: x.strip().lower())\n",
    "mergedFN= pd.merge(all_FN_df, df_annotations, on=['filename', 'page'], how='inner')\n",
    "mergedTP= pd.merge(all_TP_df, df_annotations, on=['filename', 'page'], how='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots for error analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Makes plots nicer\n",
    "sns.set(style=\"whitegrid\",font_scale=1.9)\n",
    "plt.rc('text', usetex=True)  \n",
    "plt.rc('font', family='serif') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plots the distributions of the pages for TP and FN\n",
    "sns.distplot(all_FN_df.page, label = 'False Negatives')\n",
    "sns.distplot(all_TP_df.page,label = 'True Positives')\n",
    "\n",
    "plt.ylabel('Density', fontsize = 25)\n",
    "plt.xlabel('Page',fontsize = 22, y = -2)\n",
    "plt.yticks(fontsize=15)\n",
    "plt.xticks(fontsize=15)\n",
    "plt.legend(loc='upper right',fontsize = 15)\n",
    "plt.xlim(0, 50)\n",
    "plt.savefig('page_EA.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plots the distributions of the amount of unique words for TP and FN\n",
    "sns.distplot(all_FN_df.unique_words, label = 'False Negatives')\n",
    "sns.distplot(all_TP_df.unique_words,label = 'True Positives')\n",
    "\n",
    "plt.ylabel('Density', fontsize = 25)\n",
    "plt.xlabel('Unique Words',fontsize = 22, y = -2)\n",
    "plt.yticks(fontsize=15)\n",
    "plt.xticks(fontsize=15)\n",
    "plt.legend(loc='upper left',fontsize = 15)\n",
    "plt.savefig('uniquewords_EA.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Group dataframes based on the date for plot\n",
    "count_FN_year = mergedFN.groupby(['date']).size().to_frame('count').reset_index()\n",
    "count_TP_year = mergedTP.groupby(['date']).size().to_frame('count').reset_index()\n",
    "\n",
    "#Plot the distribution of year by plotting the grouped dataframes\n",
    "sns.distplot([int(x) for x in list(all_FN_df.year) if x != 'No y'], label = 'False Negatives')\n",
    "sns.distplot([int(x) for x in list(all_TP_df.year) if x != 'No y'],label = 'True Positives')\n",
    "\n",
    "plt.ylabel('Density', fontsize = 25)\n",
    "plt.xlabel('Year',fontsize = 22, y = -2)\n",
    "plt.yticks(fontsize=15)\n",
    "plt.xticks(fontsize=15)\n",
    "plt.legend(loc='upper left',fontsize = 15)\n",
    "plt.savefig('year_EA.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Group dataframes based on the amount of highlights for plot\n",
    "count_FN_highlight = mergedFN.groupby(['filename','page']).size().to_frame('count').reset_index()\n",
    "count_TP_highlight = mergedTP.groupby(['filename','page']).size().to_frame('count').reset_index()\n",
    "\n",
    "#Plot distribution amount of highlights for FN and TP\n",
    "sns.distplot(np.array(count_FN_highlight['count']), label = 'False Negatives', bins = 10)\n",
    "sns.distplot(np.array(count_TP_highlight['count']),label = 'True Positives', bins = 13)\n",
    "\n",
    "plt.ylabel('Density', fontsize = 25, fontweight='bold')\n",
    "plt.xlabel('Amount of Highlights on a page',fontsize = 22, y = -2)\n",
    "plt.yticks(fontsize=15)\n",
    "plt.xticks(fontsize=15)\n",
    "plt.legend(loc='upper right',fontsize = 15)\n",
    "plt.savefig('highlights_EA.png', bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
