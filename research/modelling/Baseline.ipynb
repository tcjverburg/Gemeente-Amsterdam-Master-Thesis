{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import sys\n",
    "import re\n",
    "import itertools\n",
    "import operator\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "\n",
    "base = os.getcwd().split('Master-Thesis')[0].replace('\\\\', '/')\n",
    "sys.path.insert(0, base + '/Master/Thesis/Master-Thesis/research/pre-processing')\n",
    "\n",
    "from pre_processing_functions import *\n",
    "from model_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Paths for final datset\n",
    "path_dataset = base + '/Master-Thesis/research/pre-processing/final_dataset.pickle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading and defining data\n",
    "df_final= pd.read_pickle(path_dataset)\n",
    "\n",
    "#80/20 split train validation\n",
    "df_final_validation = df_final[:int(0.8*len(df_final))] \n",
    "df_test = df_final[int(0.8*len(df_final)):]             \n",
    "labels_test = df_test.check_relevant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Baseline that assumes every first page contains a highlight\n",
    "y_pred_base = []\n",
    "y_test_base = [x for  x in list(df_final_validation.check_relevant)]\n",
    "for page in list(df_final_validation.page):\n",
    "    if page == 1:\n",
    "        y_pred_base.append(True)\n",
    "    else:\n",
    "        y_pred_base.append(False)\n",
    "\n",
    "precision = precision_score(y_test_base, y_pred_base,  average=\"binary\", pos_label=True)\n",
    "recall = recall_score(y_test_base, y_pred_base,  average=\"binary\", pos_label=True)\n",
    "f1 = f1_score(y_test_base, y_pred_base, pos_label=True)\n",
    "\n",
    "table = [['Metric', 'Score'],['Precision', precision],['Recall', recall], ['F1', f1]]\n",
    "print(\"Metrics: Baseline for over the whole dataset (no test/train)\")\n",
    "display(HTML(tabulate.tabulate(table, tablefmt='html')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Baseline that assumes relevance based on key-words\n",
    "y_pred_base = []\n",
    "y_test_base = [x for  x in list(df_final_validation.check_relevant)]\n",
    "for idx,page in enumerate(list(df_final_validation.text_tokenized)):    \n",
    "    if any(word in page for word in ['algemen', 'besluit', 'bestemm', 'bepal']):\n",
    "        y_pred_base.append(True)\n",
    "    else:\n",
    "        y_pred_base.append(False)\n",
    "    \n",
    "precision = precision_score(y_test_base, y_pred_base,  average=\"binary\", pos_label=True)\n",
    "recall = recall_score(y_test_base, y_pred_base,  average=\"binary\", pos_label=True)\n",
    "f1 = f1_score(y_test_base, y_pred_base, pos_label=True)\n",
    "\n",
    "table = [['Metric', 'Score'],['Precision', precision],['Recall', recall], ['F1', f1]]\n",
    "print(\"Metrics: Baseline for over the whole dataset (no test/train)\")\n",
    "display(HTML(tabulate.tabulate(table, tablefmt='html')))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
