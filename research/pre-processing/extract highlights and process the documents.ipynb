{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/my-virtualenv-name python3\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from pathlib import Path\n",
    "import json\n",
    "import subprocess\n",
    "import sys\n",
    "import pickle\n",
    "import csv\n",
    "from pandas import DataFrame\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from pre_processing_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Different paths for data\n",
    "base = os.getcwd().split('Master-Thesis')[0].replace('\\\\', '/')\n",
    "\n",
    "#Location of the pdf files\n",
    "path_aktes = base + 'aktes'\n",
    "\n",
    "#Location of where the annotations are saved to annotations.csv. This is set in pdfannots.py under save_filename\n",
    "path_all_annotations = 'annotations.csv'\n",
    "\n",
    "#Name of the excel sheet with annotations from the system\n",
    "path_annotations_excel = 'notation_list.xlsx'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing highlights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''Reads all pdf's and extracts highlights and saves them to annotations.csv'''\n",
    "\n",
    "#Saves all filenames and failed files\n",
    "all_filenames = []\n",
    "all_fails = []\n",
    "\n",
    "for filename in os.listdir(path_aktes):\n",
    "    if filename.endswith(\".pdf\"):\n",
    "        os.rename(path_aktes + '/' + filename, path_aktes + '/' +filename.replace(' ','_'))\n",
    "        filename = filename.replace(' ','_')\n",
    "        all_filenames.append(filename)\n",
    "        try:\n",
    "            subprocess.check_output([\"python\", \"pdfannots.py\", path_aktes + '/' + filename]).decode(sys.stdout.encoding)\n",
    "        except:\n",
    "            all_fails.append(filename)\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Reads highlights from csv file and sets column names'''\n",
    "\n",
    "df_anno = pd.read_csv(path_all_annotations, encoding='latin-1')\n",
    "df_anno.columns = ['page', 'text_anno', 'file']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Perform regex on the filenames so that essential information is extracted and the data \n",
    "from excel can be linked to the individual pdf files.'''\n",
    "\n",
    "df_anno['filename'] = df_anno.apply(lambda row: row['file'].replace(' ','_'), axis = 1)\n",
    "df_anno['date'] = df_anno['file'].apply(extract_year)\n",
    "df_anno['text_tokenized_highlight'] = df_anno.apply(lambda row: stemmer(remove_stopwords(process_input\\\n",
    "                                (BeautifulSoup(row['text_anno'], 'html.parser').get_text()))), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Save annotations to pickle'''\n",
    "\n",
    "df_anno.to_pickle('complete_annotations.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process text from files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Process the documents using the pdfminer package and saves them in dataframe\n",
    "df_pdfminer = process_docs_and_label(df_anno, path_aktes, list(set(list(df_anno.file))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataframe with pages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pdfminer = df_pdfminer.reset_index()[df_pdfminer.columns]\n",
    "\n",
    "#Duplicate files in system are removed\n",
    "df_pdfminer = df_pdfminer[df_pdfminer['filename'].str.contains('.pdf_')==False] \n",
    "\n",
    "#Only overlap with annotations\n",
    "df_overlap = df_pdfminer[df_pdfminer['filename'].isin(list(df_anno.file))] \n",
    "\n",
    "#All relevant pages first\n",
    "df_overlap = df_overlap.sort_values('relevant',ascending = False) \n",
    "\n",
    "#Make string of tokenized text to find duplicates\n",
    "df_overlap['text_string'] = df_overlap.apply(lambda row: ' '.join(row['text_tokenized']), axis = 1)\n",
    "\n",
    "#Drop all false and duplicates\n",
    "df_overlap = df_overlap.drop_duplicates(subset=['text_string'], keep='first') \n",
    "\n",
    "#Lower the filenames for both dataframes\n",
    "df_overlap['filename'] = df_overlap['filename'].str.lower().str.strip()\n",
    "df_anno['filename'] = df_anno['filename'].str.lower().str.strip()\n",
    "\n",
    "#Shuffle the dataframe\n",
    "df_overlap = df_overlap.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Created merged dataframe for individual pages and highlights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check if the type of data is correct\n",
    "df_overlap['page']=df_overlap['page'].astype(int)\n",
    "df_anno['page']=df_anno['page'].astype(int)\n",
    "#Merge the individual highlights with the pages\n",
    "df_merge_col = pd.merge(df_overlap, df_anno, on=['page','filename'], how= 'left')\n",
    "#Drop duplicates\n",
    "df_merge_col= df_merge_col.drop_duplicates(subset=['filename', 'page', 'text_anno', 'text_normal'])\n",
    "df_merge_col['index'] = df_merge_col.index\n",
    "#Select the relevant columns\n",
    "df_merge_col = df_merge_col[['filename', 'text_tokenized', 'text_normal', 'page', 'relevant', 'text_anno', 'text_string']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create year column from filename\n",
    "df_merge_col['year'] = df_merge_col['filename'].apply(extract_year).str[:4]\n",
    "df_merge_col['unique_words'] = df_merge_col.apply(lambda r: len(set(r['text_tokenized'])), axis = 1)\n",
    "df_merge_col['uitgifte'] = df_merge_col.apply(lambda r: 1 if 'uitgifte' in r['filename'].lower() else 0, axis = 1)\n",
    "df_merge_col['splitsing'] = df_merge_col.apply(lambda r: 1 if 'splitsing' in r['filename'].lower() else 0, axis = 1)\n",
    "df_merge_col['levering'] = df_merge_col.apply(lambda r: 1 if 'levering' in r['filename'].lower() else 0, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter on document types and check highlights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove certain document types based on the filename\n",
    "df_merge_col = df_merge_col[~df_merge_col['filename'].str.contains('conversie')]\n",
    "df_merge_col = df_merge_col[~df_merge_col['filename'].str.contains('rectificatie')]\n",
    "df_merge_col = df_merge_col[~df_merge_col['filename'].str.contains('besluit')]\n",
    "df_merge_col = df_merge_col[~df_merge_col['filename'].str.contains('wijziging')]\n",
    "df_merge_col = df_merge_col[~df_merge_col['filename'].str.contains('samenvoeging')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check if highlight is unique on page and relates to zoning plan\n",
    "df_merge_col['check_relevant'] = df_merge_col.apply(remove_inconsistencies,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sort all pages based on whether the highlight is related to zoning plan\n",
    "df_merge_col = df_merge_col.sort_values(by = 'check_relevant', ascending = True)\n",
    "#Drop the duplicate pages and when possibl keep the page that is labelled as True \n",
    "df_merge_col = df_merge_col.drop_duplicates(subset=['filename', 'page'], keep = 'last')\n",
    "#Drop duplicate pages that have exactly the same content (even though filename is different)\n",
    "df_merge_col = df_merge_col.drop_duplicates(subset=['text_string'], keep = 'last')\n",
    "\n",
    "df_merge_col = df_merge_col.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove empty strings\n",
    "df_merge_col['text_tokenized'] = df_merge_col.apply(lambda row: list(filter(None, row['text_tokenized'])), axis = 1)\n",
    "\n",
    "#Remove words with one character or less\n",
    "df_merge_col['text_tokenized'] = df_merge_col.apply(lambda row: [word  for word in row['text_tokenized']if (len(word) > 1)] , axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create different texts for the different vectorization methods\n",
    "df_merge_col['text_tokenized_joined'] =  df_merge_col.apply(lambda row: ' '.join(row['text_tokenized']), axis = 1)\n",
    "\n",
    "df_merge_col['tokenized_unstemmed_unstopwords_all'] = df_merge_col.apply(lambda row: remove_names(process_input\\\n",
    "                               (BeautifulSoup(row['text_normal'], 'html.parser').get_text())), axis = 1)\n",
    "\n",
    "df_merge_col['text_ngrams'] = df_merge_col.apply(lambda row: ' '.join(row['tokenized_unstemmed_unstopwords_all']), axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the pages of certain document types and shuffle the dataframe randomly\n",
    "df_merge_col = df_merge_col[(df_merge_col.levering == 1)|(df_merge_col.splitsing == 1)|(df_merge_col.uitgifte == 1)]\n",
    "df_merge_col = shuffle(df_merge_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save dataframe\n",
    "df_merge_col.to_pickle('final_dataset.pickle')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
